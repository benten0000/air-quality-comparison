{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5800594f",
   "metadata": {},
   "source": [
    "# Air quality Cross-Station Transfer Learning using geo aware approach \n",
    "\n",
    "Protocol:\n",
    "- Train: first **365 days**\n",
    "- Val: next **30 days**\n",
    "- Test: remainder\n",
    "- Reduced targets: **last 6 months of the 1-year training interval** (val/test unchanged)\n",
    "\n",
    "Methods:\n",
    "- Scratch_Univar\n",
    "- TL_Univar\n",
    "- Hybrid_TL_Univar\n",
    "- Scratch_Multivar\n",
    "- TL_Multivar\n",
    "- Hybrid_TL_Multivar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e902dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, time, json, re, random, warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import mlflow\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -----------------------\n",
    "# Env + MLflow\n",
    "# -----------------------\n",
    "load_dotenv()\n",
    "mlflow.set_tracking_uri(\"https://mlflow.stack.grega.xyz/\")\n",
    "EXPERIMENT_NAME = \"AirQuality_CrossStation_TrainMonthsSweep_TL_Torch\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "# -----------------------\n",
    "# Repro\n",
    "# -----------------------\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "DATA_DIR = \"data/processed/combined\"\n",
    "GEO_JSON_PATH = \"data/processed/stations_geo.json\"   # change if needed\n",
    "\n",
    "TRAIN_DAYS = 365\n",
    "VAL_DAYS = 30\n",
    "TRAIN_MONTHS_SWEEP = list(range(1, 13))\n",
    "\n",
    "SEQ_LEN = 24\n",
    "HORIZON = 1\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCHS = 200\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 0.0\n",
    "\n",
    "EARLY_STOP_PATIENCE = 10\n",
    "REDUCE_LR_PATIENCE = 5\n",
    "\n",
    "LSTM_UNITS = 48\n",
    "EMBED_DIM = 16\n",
    "GEO_DIM = 2\n",
    "\n",
    "RESULTS_DIR = \"exp_results\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "DEFAULT_METEO_NUM_COLS = [\"temperature\", \"rain\", \"pressure\", \"precipitation\", \"wind_speed\"]\n",
    "CAL_COLS = [\"hour_sin\", \"hour_cos\", \"dow_sin\", \"dow_cos\", \"doy_sin\", \"doy_cos\"]\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Helpers: geo loader\n",
    "# ============================================================\n",
    "def load_geo_map(path: str) -> Dict[str, Dict[str, float]]:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Geo file not found: {path}\")\n",
    "    txt = open(path, \"r\", encoding=\"utf-8\").read().strip()\n",
    "    try:\n",
    "        geo_list = json.loads(txt)\n",
    "    except json.JSONDecodeError:\n",
    "        txt2 = re.sub(r'([\\{\\s,])([A-Za-z_][A-Za-z0-9_]*)(\\s*):', r'\\1\"\\2\"\\3:', txt)\n",
    "        txt2 = re.sub(r\"'\", '\"', txt2)\n",
    "        txt2 = re.sub(r\",\\s*([}\\]])\", r\"\\1\", txt2)\n",
    "        geo_list = json.loads(txt2)\n",
    "\n",
    "    geo = {}\n",
    "    for s in geo_list:\n",
    "        serial = s.get(\"serial\")\n",
    "        lat = s.get(\"latitude\")\n",
    "        lon = s.get(\"longitude\")\n",
    "        label = s.get(\"label\", serial)\n",
    "        if serial is None or lat is None or lon is None:\n",
    "            continue\n",
    "        geo[str(serial)] = {\"lat\": float(lat), \"lon\": float(lon), \"label\": str(label)}\n",
    "    if not geo:\n",
    "        raise ValueError(\"No valid geo stations found.\")\n",
    "    return geo\n",
    "\n",
    "geo_map = load_geo_map(GEO_JSON_PATH)\n",
    "\n",
    "# ============================================================\n",
    "# Helpers: load + features + leakage-free split/impute\n",
    "# ============================================================\n",
    "def list_station_files(data_dir: str) -> List[str]:\n",
    "    files = sorted(glob.glob(os.path.join(data_dir, \"*.csv\")))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No CSVs in {data_dir}\")\n",
    "    return files\n",
    "\n",
    "def station_name_from_path(p: str) -> str:\n",
    "    return os.path.splitext(os.path.basename(p))[0]\n",
    "\n",
    "def load_station_csv(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "    df = df.sort_values(\"datetime\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def add_calendar_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    dt = d[\"datetime\"]\n",
    "    hour = dt.dt.hour.values\n",
    "    dow  = dt.dt.dayofweek.values\n",
    "    doy  = dt.dt.dayofyear.values\n",
    "    d[\"hour_sin\"] = np.sin(2*np.pi*hour/24.0)\n",
    "    d[\"hour_cos\"] = np.cos(2*np.pi*hour/24.0)\n",
    "    d[\"dow_sin\"]  = np.sin(2*np.pi*dow/7.0)\n",
    "    d[\"dow_cos\"]  = np.cos(2*np.pi*dow/7.0)\n",
    "    d[\"doy_sin\"]  = np.sin(2*np.pi*doy/365.25)\n",
    "    d[\"doy_cos\"]  = np.cos(2*np.pi*doy/365.25)\n",
    "    return d\n",
    "\n",
    "def split_by_time_spans(df: pd.DataFrame, train_days: int, val_days: int):\n",
    "    start = df[\"datetime\"].min()\n",
    "    train_end = start + pd.Timedelta(days=train_days)\n",
    "    val_end = train_end + pd.Timedelta(days=val_days)\n",
    "    tr = df[df[\"datetime\"] < train_end].copy()\n",
    "    va = df[(df[\"datetime\"] >= train_end) & (df[\"datetime\"] < val_end)].copy()\n",
    "    te = df[df[\"datetime\"] >= val_end].copy()\n",
    "    # Basic length checks (prevents empty window tensors)\n",
    "    min_train_points = (SEQ_LEN + HORIZON) + 24  # at least ~1 day beyond a single window\n",
    "    min_val_points = (SEQ_LEN + HORIZON) + 24\n",
    "    if len(tr) < min_train_points:\n",
    "        raise ValueError(f\"Train split too short: {len(tr)} rows\")\n",
    "    if len(va) < min_val_points:\n",
    "        raise ValueError(f\"Val split too short: {len(va)} rows\")\n",
    "    if len(te) < (SEQ_LEN + HORIZON):\n",
    "        raise ValueError(f\"Test split too short: {len(te)} rows\")\n",
    "    return tr.reset_index(drop=True), va.reset_index(drop=True), te.reset_index(drop=True)\n",
    "\n",
    "def fill_missing_time_split_local(df: pd.DataFrame, cols: List[str]) -> pd.DataFrame:\n",
    "    d = df.copy().set_index(\"datetime\")\n",
    "    d[cols] = d[cols].interpolate(method=\"time\", limit_direction=\"both\")\n",
    "    d[cols] = d[cols].ffill().bfill()\n",
    "    return d.reset_index()\n",
    "\n",
    "def reduce_train_last_months(train_df: pd.DataFrame, months: int = 6) -> pd.DataFrame:\n",
    "    end_excl = train_df[\"datetime\"].max() + pd.Timedelta(hours=1)\n",
    "    start = end_excl - pd.DateOffset(months=months)\n",
    "    red = train_df[train_df[\"datetime\"] >= start].copy()\n",
    "    return red.reset_index(drop=True)\n",
    "\n",
    "def dyn_cols_meteo_cal(meteo_cols: List[str]) -> List[str]:\n",
    "    return [\"PM10\"] + meteo_cols + CAL_COLS\n",
    "\n",
    "def prepare_station_record(path: str) -> Dict[str, object]:\n",
    "    st = station_name_from_path(path)\n",
    "    df = load_station_csv(path)\n",
    "    df = add_calendar_features(df)\n",
    "    meteo_cols = [c for c in DEFAULT_METEO_NUM_COLS if c in df.columns]\n",
    "\n",
    "    # leakage-safe: split first, then impute within each split\n",
    "    tr, va, te = split_by_time_spans(df, TRAIN_DAYS, VAL_DAYS)\n",
    "    cont_cols = [\"PM10\"] + ([\"PM2.5\"] if \"PM2.5\" in df.columns else []) + meteo_cols\n",
    "    tr = fill_missing_time_split_local(tr, cont_cols)\n",
    "    va = fill_missing_time_split_local(va, cont_cols)\n",
    "    te = fill_missing_time_split_local(te, cont_cols)\n",
    "\n",
    "    return {\"station\": st, \"df_train\": tr, \"df_val\": va, \"df_test\": te, \"meteo_cols\": meteo_cols}\n",
    "\n",
    "station_files = list_station_files(DATA_DIR)\n",
    "stations = [station_name_from_path(p) for p in station_files]\n",
    "station_to_id = {s:i for i,s in enumerate(stations)}\n",
    "num_stations = len(stations)\n",
    "\n",
    "station_data = [prepare_station_record(p) for p in station_files]\n",
    "\n",
    "# common meteo columns across all stations\n",
    "common = sorted(list(set.intersection(*[set(d[\"meteo_cols\"]) for d in station_data])))\n",
    "X_COLS = dyn_cols_meteo_cal(common)\n",
    "\n",
    "# Univariate feature set (PM10-only)\n",
    "X_COLS_UNIV = [\"PM10\"]\n",
    "\n",
    "# Multivariate feature set\n",
    "X_COLS_MULT = X_COLS\n",
    "\n",
    "METHODS = [\"Scratch\", \"TL\", \"GeoAware_TL\"]\n",
    "\n",
    "print(\"Stations:\", len(stations), \"Common meteo:\", common)\n",
    "\n",
    "_missing_geo_logged = set()\n",
    "\n",
    "def geo_vec(station: str) -> np.ndarray:\n",
    "    g = geo_map.get(station)\n",
    "    if g is None:\n",
    "        if station not in _missing_geo_logged:\n",
    "            warnings.warn(f\"Missing geo for station={station}; using zeros\")\n",
    "            _missing_geo_logged.add(station)\n",
    "        return np.zeros((2,), dtype=np.float32)\n",
    "    return np.array([g[\"lat\"], g[\"lon\"]], dtype=np.float32)\n",
    "\n",
    "geo_vec_map = {s: geo_vec(s) for s in stations}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Scaling + windowing\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class Scalers:\n",
    "    x_mean: np.ndarray\n",
    "    x_std: np.ndarray\n",
    "    y_mean: float\n",
    "    y_std: float\n",
    "\n",
    "def fit_scalers(train_df: pd.DataFrame, x_cols: List[str], y_col: str = \"PM10\") -> Scalers:\n",
    "    X = train_df[x_cols].values.astype(np.float32)\n",
    "    y = train_df[y_col].values.astype(np.float32)\n",
    "    return Scalers(\n",
    "        x_mean=X.mean(axis=0),\n",
    "        x_std=X.std(axis=0) + 1e-8,\n",
    "        y_mean=float(y.mean()),\n",
    "        y_std=float(y.std() + 1e-8),\n",
    "    )\n",
    "\n",
    "def apply_scalers(df: pd.DataFrame, sc: Scalers, x_cols: List[str], y_col: str = \"PM10\") -> pd.DataFrame:\n",
    "    d = df.copy()\n",
    "    d[x_cols] = (d[x_cols].values.astype(np.float32) - sc.x_mean) / sc.x_std\n",
    "    d[f\"{y_col}_y\"] = (d[y_col].values.astype(np.float32) - sc.y_mean) / sc.y_std\n",
    "    return d\n",
    "\n",
    "def make_windows(df: pd.DataFrame, x_cols: List[str], y_scaled_col: str, seq_len: int, horizon: int):\n",
    "    Xv = df[x_cols].values.astype(np.float32)\n",
    "    yv = df[y_scaled_col].values.astype(np.float32)\n",
    "    X, Y = [], []\n",
    "    for end in range(seq_len, len(df) - horizon + 1):\n",
    "        X.append(Xv[end-seq_len:end])\n",
    "        Y.append(yv[end + horizon - 1])\n",
    "    return np.stack(X, axis=0), np.array(Y, dtype=np.float32).reshape(-1, 1)\n",
    "\n",
    "def build_station_windows(station_record, x_cols: List[str], train_df_override: Optional[pd.DataFrame] = None):\n",
    "    tr = train_df_override if train_df_override is not None else station_record[\"df_train\"]\n",
    "    va, te = station_record[\"df_val\"], station_record[\"df_test\"]\n",
    "\n",
    "    sc = fit_scalers(tr, x_cols, \"PM10\")\n",
    "    tr_s = apply_scalers(tr, sc, x_cols, \"PM10\")\n",
    "    va_s = apply_scalers(va, sc, x_cols, \"PM10\")\n",
    "    te_s = apply_scalers(te, sc, x_cols, \"PM10\")\n",
    "\n",
    "    Xtr, ytr = make_windows(tr_s, x_cols, \"PM10_y\", SEQ_LEN, HORIZON)\n",
    "    Xva, yva = make_windows(va_s, x_cols, \"PM10_y\", SEQ_LEN, HORIZON)\n",
    "    Xte, yte = make_windows(te_s, x_cols, \"PM10_y\", SEQ_LEN, HORIZON)\n",
    "    tte = te_s[\"datetime\"].iloc[SEQ_LEN + HORIZON - 1:].reset_index(drop=True)\n",
    "    return (Xtr, ytr), (Xva, yva), (Xte, yte), tte, sc\n",
    "\n",
    "def inverse_y(y_scaled: np.ndarray, sc: Scalers) -> np.ndarray:\n",
    "    return y_scaled * sc.y_std + sc.y_mean\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    y_true = y_true.reshape(-1)\n",
    "    y_pred = y_pred.reshape(-1)\n",
    "    err = y_true - y_pred\n",
    "    mae = float(np.mean(np.abs(err)))\n",
    "    mse = float(np.mean(err ** 2))\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    denom = np.clip(np.abs(y_true), 1e-8, None)\n",
    "    mape = float(np.mean(np.abs(err) / denom) * 100.0)\n",
    "    smape = float(np.mean(2.0 * np.abs(err) / np.clip(np.abs(y_true) + np.abs(y_pred), 1e-8, None)) * 100.0)\n",
    "    y_var = float(np.var(y_true))\n",
    "    r2 = float(1.0 - (np.sum(err ** 2) / np.clip(np.sum((y_true - np.mean(y_true)) ** 2), 1e-8, None)))\n",
    "    evs = float(1.0 - (np.var(err) / np.clip(y_var, 1e-8, None)))\n",
    "    return {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"MAPE\": mape, \"sMAPE\": smape, \"R2\": r2, \"EVS\": evs}\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Models\n",
    "# ============================================================\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, n_features: int, hidden: int):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.fc(h)\n",
    "\n",
    "class HybridLocLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    dyn_seq: [B,T,F]\n",
    "    station_id: [B] long\n",
    "    geo: [B,2] float\n",
    "    \"\"\"\n",
    "    def __init__(self, n_dyn_features: int, hidden: int, num_stations: int, embed_dim: int, geo_dim: int = 2):\n",
    "        super().__init__()\n",
    "        self.station_emb = nn.Embedding(num_stations, embed_dim)\n",
    "        self.geo_mlp = nn.Sequential(\n",
    "            nn.Linear(geo_dim, 16), nn.ReLU(),\n",
    "            nn.Linear(16, embed_dim), nn.ReLU()\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=n_dyn_features + embed_dim, hidden_size=hidden, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden, 1)\n",
    "\n",
    "    def forward(self, dyn_seq, station_id, geo):\n",
    "        e_id = self.station_emb(station_id)          # [B,E]\n",
    "        e_geo = self.geo_mlp(geo)                    # [B,E]\n",
    "        e = e_id + e_geo                             # [B,E]\n",
    "        e_seq = e.unsqueeze(1).expand(-1, dyn_seq.size(1), -1)  # [B,T,E]\n",
    "        x = torch.cat([dyn_seq, e_seq], dim=-1)      # [B,T,F+E]\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]\n",
    "        return self.fc(h)\n",
    "\n",
    "def freeze_lstm_layer0(model: nn.Module):\n",
    "    for name, p in model.lstm.named_parameters():\n",
    "        if \"l0\" in name:\n",
    "            p.requires_grad = False\n",
    "    return model\n",
    "\n",
    "def build_backbone_model(n_features: int) -> LSTMRegressor:\n",
    "    return LSTMRegressor(n_features=n_features, hidden=LSTM_UNITS).to(device)\n",
    "\n",
    "def build_hybrid_model(n_dyn_features: int) -> HybridLocLSTM:\n",
    "    return HybridLocLSTM(\n",
    "        n_dyn_features=n_dyn_features,\n",
    "        hidden=LSTM_UNITS,\n",
    "        num_stations=num_stations,\n",
    "        embed_dim=EMBED_DIM,\n",
    "        geo_dim=GEO_DIM,\n",
    "    ).to(device)\n",
    "\n",
    "def warm_start_station_embedding_from_source(model: HybridLocLSTM, source_station: str, target_station: str):\n",
    "    with torch.no_grad():\n",
    "        sid = station_to_id[source_station]\n",
    "        tid = station_to_id[target_station]\n",
    "        model.station_emb.weight[tid].copy_(model.station_emb.weight[sid])\n",
    "    return model\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Training utilities (early stopping + reduce LR)\n",
    "# ============================================================\n",
    "def make_loader(X, y, batch_size, shuffle):\n",
    "    ds = TensorDataset(torch.from_numpy(X), torch.from_numpy(y))\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "def train_loop(model, train_loader, val_loader, optimizer, scheduler, max_epochs, patience, run_name: str,\n",
    "               hybrid: bool = False, station_id=None, geo=None):\n",
    "    \"\"\"\n",
    "    hybrid=False expects batch: (X,y)\n",
    "    hybrid=True expects dyn_seq from X, but station_id and geo are provided as full tensors aligned with X\n",
    "    \"\"\"\n",
    "    best_val = float(\"inf\")\n",
    "    bad = 0\n",
    "    best_state = None\n",
    "    t0 = time.time()\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if not hybrid:\n",
    "                pred = model(xb)\n",
    "            else:\n",
    "                # station_id, geo must be tensors aligned to dataset order\n",
    "                # We reconstruct by indexing the full tensors using batch indices is cumbersome with DataLoader,\n",
    "                # so for hybrid we pass loaders that already include station_id and geo.\n",
    "                raise RuntimeError(\"Use train_loop_hybrid for hybrid models.\")\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # val\n",
    "        model.eval()\n",
    "        vloss = 0.0\n",
    "        n = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(device)\n",
    "                yb = yb.to(device)\n",
    "                pred = model(xb)\n",
    "                l = loss_fn(pred, yb).item()\n",
    "                vloss += l * len(xb)\n",
    "                n += len(xb)\n",
    "        vloss /= max(1, n)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(vloss)\n",
    "\n",
    "        if vloss + 1e-12 < best_val:\n",
    "            best_val = vloss\n",
    "            bad = 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "\n",
    "    return time.time() - t0, best_val\n",
    "\n",
    "def make_hybrid_loader(X, y, station_ids, geos, batch_size, shuffle):\n",
    "    ds = TensorDataset(\n",
    "        torch.from_numpy(X),\n",
    "        torch.from_numpy(y),\n",
    "        torch.from_numpy(station_ids.astype(np.int64)),\n",
    "        torch.from_numpy(geos.astype(np.float32)),\n",
    "    )\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=False)\n",
    "\n",
    "def train_loop_hybrid(model, train_loader, val_loader, optimizer, scheduler, max_epochs, patience, run_name: str):\n",
    "    best_val = float(\"inf\")\n",
    "    bad = 0\n",
    "    best_state = None\n",
    "    t0 = time.time()\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        model.train()\n",
    "        for xb, yb, sidb, geob in train_loader:\n",
    "            xb = xb.to(device); yb = yb.to(device)\n",
    "            sidb = sidb.to(device); geob = geob.to(device)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            pred = model(xb, sidb, geob)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # val\n",
    "        model.eval()\n",
    "        vloss = 0.0\n",
    "        n = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb, sidb, geob in val_loader:\n",
    "                xb = xb.to(device); yb = yb.to(device)\n",
    "                sidb = sidb.to(device); geob = geob.to(device)\n",
    "                pred = model(xb, sidb, geob)\n",
    "                l = loss_fn(pred, yb).item()\n",
    "                vloss += l * len(xb)\n",
    "                n += len(xb)\n",
    "        vloss /= max(1, n)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(vloss)\n",
    "\n",
    "        if vloss + 1e-12 < best_val:\n",
    "            best_val = vloss\n",
    "            bad = 0\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "\n",
    "    return time.time() - t0, best_val\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict(model, loader, hybrid: bool = False):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    if not hybrid:\n",
    "        for xb, yb in loader:\n",
    "            xb = xb.to(device)\n",
    "            pred = model(xb).detach().cpu().numpy()\n",
    "            preds.append(pred)\n",
    "    else:\n",
    "        for xb, yb, sidb, geob in loader:\n",
    "            xb = xb.to(device); sidb = sidb.to(device); geob = geob.to(device)\n",
    "            pred = model(xb, sidb, geob).detach().cpu().numpy()\n",
    "            preds.append(pred)\n",
    "    return np.vstack(preds)\n",
    "\n",
    "def log_torch_model(model: nn.Module, artifact_path: str, fname: str = \"model.pt\"):\n",
    "    os.makedirs(artifact_path, exist_ok=True)\n",
    "    path = os.path.join(artifact_path, fname)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    mlflow.log_artifact(path, artifact_path=os.path.basename(artifact_path))\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# Core: source training + target eval\n",
    "# ============================================================\n",
    "def train_source_models(source_record):\n",
    "    src = source_record[\"station\"]\n",
    "    pack = {\"src_station\": src}\n",
    "\n",
    "    (Xtr, ytr), (Xva, yva), _, _, _ = build_station_windows(source_record, X_COLS_MULT)\n",
    "\n",
    "    m_backbone = build_backbone_model(n_features=Xtr.shape[-1])\n",
    "    opt = torch.optim.Adam(m_backbone.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=REDUCE_LR_PATIENCE, min_lr=1e-6)\n",
    "    tr_loader = make_loader(Xtr, ytr, BATCH_SIZE, shuffle=True)\n",
    "    va_loader = make_loader(Xva, yva, BATCH_SIZE, shuffle=False)\n",
    "    dt_backbone, _ = train_loop(\n",
    "        m_backbone, tr_loader, va_loader, opt, sch,\n",
    "        MAX_EPOCHS, EARLY_STOP_PATIENCE, run_name=f\"src_backbone_{src}\"\n",
    "    )\n",
    "\n",
    "    sid_tr = np.full((len(Xtr),), station_to_id[src], dtype=np.int64)\n",
    "    sid_va = np.full((len(Xva),), station_to_id[src], dtype=np.int64)\n",
    "    geo = geo_vec_map[src].astype(np.float32)\n",
    "    geo_tr = np.tile(geo.reshape(1, -1), (len(Xtr), 1))\n",
    "    geo_va = np.tile(geo.reshape(1, -1), (len(Xva), 1))\n",
    "\n",
    "    m_hybrid = build_hybrid_model(n_dyn_features=Xtr.shape[-1])\n",
    "    opt2 = torch.optim.Adam(m_hybrid.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    sch2 = torch.optim.lr_scheduler.ReduceLROnPlateau(opt2, factor=0.5, patience=REDUCE_LR_PATIENCE, min_lr=1e-6)\n",
    "    tr_loader2 = make_hybrid_loader(Xtr, ytr, sid_tr, geo_tr, BATCH_SIZE, shuffle=True)\n",
    "    va_loader2 = make_hybrid_loader(Xva, yva, sid_va, geo_va, BATCH_SIZE, shuffle=False)\n",
    "    dt_hybrid, _ = train_loop_hybrid(\n",
    "        m_hybrid, tr_loader2, va_loader2, opt2, sch2,\n",
    "        MAX_EPOCHS, EARLY_STOP_PATIENCE, run_name=f\"src_hybrid_{src}\"\n",
    "    )\n",
    "\n",
    "    pack.update({\n",
    "        \"backbone_model\": m_backbone,\n",
    "        \"backbone_train_time\": dt_backbone,\n",
    "        \"hybrid_model\": m_hybrid,\n",
    "        \"hybrid_train_time\": dt_hybrid,\n",
    "    })\n",
    "    return pack\n",
    "\n",
    "def log_common_params(source_station: str, target_station: str, method: str,\n",
    "                      x_cols: List[str], train_windows_full: int, train_windows_used: int,\n",
    "                      train_months_used: int, target_col: str, target_name: str):\n",
    "    mlflow.log_params({\n",
    "        \"source_station\": source_station,\n",
    "        \"target_station\": target_station,\n",
    "        \"target_name\": target_name,\n",
    "        \"target_col\": target_col,\n",
    "        \"method\": method,\n",
    "        \"feature_set\": \"univariate\" if x_cols == [\"PM10\"] else \"multivariate\",\n",
    "        \"features\": \",\".join(x_cols),\n",
    "\n",
    "        \"train_days\": TRAIN_DAYS,\n",
    "        \"val_days\": VAL_DAYS,\n",
    "        \"train_months_used\": int(train_months_used),\n",
    "        \"reduction_mode\": \"last_months\",\n",
    "\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"horizon\": HORIZON,\n",
    "\n",
    "        \"lstm_units\": LSTM_UNITS,\n",
    "        \"embed_dim\": EMBED_DIM,\n",
    "        \"lr\": LR,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "\n",
    "        \"train_windows_full\": train_windows_full,\n",
    "        \"train_windows_used\": train_windows_used,\n",
    "\n",
    "        \"device\": str(device),\n",
    "\n",
    "        \"lat\": float(geo_vec_map[target_station][0]),\n",
    "        \"lon\": float(geo_vec_map[target_station][1]),\n",
    "    })\n",
    "\n",
    "def log_predictions_artifact(\n",
    "    pred_df: pd.DataFrame,\n",
    "    source_station: str,\n",
    "    target_station: str,\n",
    "    method: str,\n",
    "    train_months_used: int,\n",
    "    artifact_root: str = \"predictions\",\n",
    "):\n",
    "    tmp_dir = os.path.join(RESULTS_DIR, \"tmp_predictions\")\n",
    "    os.makedirs(tmp_dir, exist_ok=True)\n",
    "    method_safe = method.replace(\"/\", \"_\")\n",
    "    fname = f\"pred_{source_station}_to_{target_station}_{method_safe}_k{train_months_used}.csv\"\n",
    "    path = os.path.join(tmp_dir, fname)\n",
    "    pred_df.to_csv(path, index=False)\n",
    "    mlflow.log_artifact(path, artifact_path=f\"{artifact_root}/k={train_months_used}/{method_safe}\")\n",
    "    return path\n",
    "\n",
    "def run_one_method(\n",
    "    method: str,\n",
    "    source_station: str,\n",
    "    src_pack: dict,\n",
    "    target_record: dict,\n",
    "    target_name: str,\n",
    "    target_col: str,\n",
    "    train_months_used: int,\n",
    "    x_cols: List[str],\n",
    "    train_windows_full: int,\n",
    "    train_windows_used: int,\n",
    "    window_pack: dict,\n",
    "):\n",
    "    tgt = target_record[\"station\"]\n",
    "    Xtr, ytr = window_pack[\"Xtr\"], window_pack[\"ytr\"]\n",
    "    Xva, yva = window_pack[\"Xva\"], window_pack[\"yva\"]\n",
    "    Xte, yte = window_pack[\"Xte\"], window_pack[\"yte\"]\n",
    "    tte, sc, yt = window_pack[\"tte\"], window_pack[\"sc\"], window_pack[\"yt\"]\n",
    "\n",
    "    run_name = f\"method={method}_k={train_months_used}\"\n",
    "    with mlflow.start_run(run_name=run_name, nested=True):\n",
    "        log_common_params(\n",
    "            source_station=source_station,\n",
    "            target_station=tgt,\n",
    "            method=method,\n",
    "            x_cols=x_cols,\n",
    "            train_windows_full=train_windows_full,\n",
    "            train_windows_used=train_windows_used,\n",
    "            train_months_used=train_months_used,\n",
    "            target_col=target_col,\n",
    "            target_name=target_name,\n",
    "        )\n",
    "\n",
    "        if method == \"Scratch\":\n",
    "            model = build_backbone_model(n_features=Xtr.shape[-1])\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=REDUCE_LR_PATIENCE, min_lr=1e-6)\n",
    "            tr_loader = make_loader(Xtr, ytr, BATCH_SIZE, shuffle=True)\n",
    "            va_loader = make_loader(Xva, yva, BATCH_SIZE, shuffle=False)\n",
    "            te_loader = make_loader(Xte, yte, BATCH_SIZE, shuffle=False)\n",
    "            dt, _ = train_loop(\n",
    "                model, tr_loader, va_loader, opt, sch,\n",
    "                MAX_EPOCHS, EARLY_STOP_PATIENCE, run_name=f\"scratch_{tgt}_k{train_months_used}\"\n",
    "            )\n",
    "            yp = inverse_y(predict(model, te_loader, hybrid=False), sc)\n",
    "\n",
    "        elif method == \"TL\":\n",
    "            model = build_backbone_model(n_features=Xtr.shape[-1])\n",
    "            model.load_state_dict(src_pack[\"backbone_model\"].state_dict())\n",
    "            freeze_lstm_layer0(model)\n",
    "            opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=REDUCE_LR_PATIENCE, min_lr=1e-6)\n",
    "            tr_loader = make_loader(Xtr, ytr, BATCH_SIZE, shuffle=True)\n",
    "            va_loader = make_loader(Xva, yva, BATCH_SIZE, shuffle=False)\n",
    "            te_loader = make_loader(Xte, yte, BATCH_SIZE, shuffle=False)\n",
    "            dt, _ = train_loop(\n",
    "                model, tr_loader, va_loader, opt, sch,\n",
    "                MAX_EPOCHS, EARLY_STOP_PATIENCE, run_name=f\"tl_{tgt}_k{train_months_used}\"\n",
    "            )\n",
    "            yp = inverse_y(predict(model, te_loader, hybrid=False), sc)\n",
    "\n",
    "        elif method == \"GeoAware_TL\":\n",
    "            model = build_hybrid_model(n_dyn_features=Xtr.shape[-1])\n",
    "            model.load_state_dict(src_pack[\"hybrid_model\"].state_dict())\n",
    "            if tgt != source_station:\n",
    "                warm_start_station_embedding_from_source(model, source_station, tgt)\n",
    "            freeze_lstm_layer0(model)\n",
    "            mlflow.log_param(\"warm_start_embedding\", True)\n",
    "\n",
    "            sid_tr = np.full((len(Xtr),), station_to_id[tgt], dtype=np.int64)\n",
    "            sid_va = np.full((len(Xva),), station_to_id[tgt], dtype=np.int64)\n",
    "            sid_te = np.full((len(Xte),), station_to_id[tgt], dtype=np.int64)\n",
    "            geo = geo_vec_map[tgt].astype(np.float32)\n",
    "            geo_tr = np.tile(geo.reshape(1, -1), (len(Xtr), 1))\n",
    "            geo_va = np.tile(geo.reshape(1, -1), (len(Xva), 1))\n",
    "            geo_te = np.tile(geo.reshape(1, -1), (len(Xte), 1))\n",
    "\n",
    "            tr_loader = make_hybrid_loader(Xtr, ytr, sid_tr, geo_tr, BATCH_SIZE, shuffle=True)\n",
    "            va_loader = make_hybrid_loader(Xva, yva, sid_va, geo_va, BATCH_SIZE, shuffle=False)\n",
    "            te_loader = make_hybrid_loader(Xte, yte, sid_te, geo_te, BATCH_SIZE, shuffle=False)\n",
    "            opt = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "            sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, factor=0.5, patience=REDUCE_LR_PATIENCE, min_lr=1e-6)\n",
    "            dt, _ = train_loop_hybrid(\n",
    "                model, tr_loader, va_loader, opt, sch,\n",
    "                MAX_EPOCHS, EARLY_STOP_PATIENCE, run_name=f\"geoaware_tl_{tgt}_k{train_months_used}\"\n",
    "            )\n",
    "            yp = inverse_y(predict(model, te_loader, hybrid=True), sc)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        m = compute_metrics(yt, yp)\n",
    "        mlflow.log_metrics({\n",
    "            \"mae\": m[\"MAE\"],\n",
    "            \"rmse\": m[\"RMSE\"],\n",
    "            \"mse\": m[\"MSE\"],\n",
    "            \"mape\": m[\"MAPE\"],\n",
    "            \"smape\": m[\"sMAPE\"],\n",
    "            \"r2\": m[\"R2\"],\n",
    "            \"evs\": m[\"EVS\"],\n",
    "            \"train_time_sec\": float(dt),\n",
    "        })\n",
    "\n",
    "        tmp_dir = os.path.join(RESULTS_DIR, \"tmp_models\")\n",
    "        os.makedirs(tmp_dir, exist_ok=True)\n",
    "        model_path = os.path.join(tmp_dir, f\"{source_station}_to_{tgt}_{method}_k{train_months_used}.pt\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "        mlflow.log_artifact(model_path, artifact_path=f\"model/k={train_months_used}/{method}\")\n",
    "\n",
    "        pred_df = pd.DataFrame({\n",
    "            \"datetime\": pd.to_datetime(tte).astype(str),\n",
    "            \"y_true\": yt.reshape(-1),\n",
    "            \"y_pred\": yp.reshape(-1),\n",
    "            \"source_station\": source_station,\n",
    "            \"target_station\": tgt,\n",
    "            \"method\": method,\n",
    "            \"train_months_used\": int(train_months_used),\n",
    "            \"target_col\": target_col,\n",
    "            \"target_name\": target_name,\n",
    "        })\n",
    "        log_predictions_artifact(\n",
    "            pred_df=pred_df,\n",
    "            source_station=source_station,\n",
    "            target_station=tgt,\n",
    "            method=method,\n",
    "            train_months_used=train_months_used,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"TargetName\": target_name,\n",
    "            \"TargetCol\": target_col,\n",
    "            \"SourceStation\": source_station,\n",
    "            \"TargetStation\": tgt,\n",
    "            \"Method\": method,\n",
    "            \"TrainMonthsUsed\": int(train_months_used),\n",
    "            **m,\n",
    "            \"TrainTimeSec\": float(dt),\n",
    "        }\n",
    "\n",
    "def eval_target_sweep(source_station: str, src_pack: dict, target_record: dict):\n",
    "    tgt = target_record[\"station\"]\n",
    "    target_col = \"PM10\"\n",
    "    target_name = geo_map.get(tgt, {}).get(\"label\", tgt)\n",
    "\n",
    "    train_full = target_record[\"df_train\"]\n",
    "    train_windows_full = max(0, len(train_full) - SEQ_LEN - HORIZON + 1)\n",
    "    rows = []\n",
    "    skipped_months = []\n",
    "    window_cache = {}\n",
    "\n",
    "    with mlflow.start_run(run_name=f\"target={tgt}\", nested=True):\n",
    "        for k in TRAIN_MONTHS_SWEEP:\n",
    "            cache_key = (tgt, int(k), target_col)\n",
    "            train_reduced = reduce_train_last_months(train_full, months=int(k))\n",
    "            train_windows_used = max(0, len(train_reduced) - SEQ_LEN - HORIZON + 1)\n",
    "\n",
    "            with mlflow.start_run(run_name=f\"k={k}\", nested=True):\n",
    "                if cache_key not in window_cache:\n",
    "                    try:\n",
    "                        (Xtr, ytr), (Xva, yva), (Xte, yte), tte, sc = build_station_windows(\n",
    "                            target_record, X_COLS_MULT, train_df_override=train_reduced\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        warnings.warn(f\"Skipping target={tgt} k={k}: failed windowing ({e})\")\n",
    "                        skipped_months.append(int(k))\n",
    "                        window_cache[cache_key] = None\n",
    "                        continue\n",
    "\n",
    "                    if len(Xtr) == 0 or len(Xva) == 0 or len(Xte) == 0:\n",
    "                        warnings.warn(f\"Skipping target={tgt} k={k}: empty windows (tr={len(Xtr)}, va={len(Xva)}, te={len(Xte)})\")\n",
    "                        skipped_months.append(int(k))\n",
    "                        window_cache[cache_key] = None\n",
    "                        continue\n",
    "\n",
    "                    yt = inverse_y(yte, sc)\n",
    "                    window_cache[cache_key] = {\n",
    "                        \"Xtr\": Xtr, \"ytr\": ytr,\n",
    "                        \"Xva\": Xva, \"yva\": yva,\n",
    "                        \"Xte\": Xte, \"yte\": yte,\n",
    "                        \"tte\": tte, \"sc\": sc, \"yt\": yt,\n",
    "                    }\n",
    "\n",
    "                pack = window_cache[cache_key]\n",
    "                if pack is None:\n",
    "                    continue\n",
    "\n",
    "                for method in METHODS:\n",
    "                    rows.append(\n",
    "                        run_one_method(\n",
    "                            method=method,\n",
    "                            source_station=source_station,\n",
    "                            src_pack=src_pack,\n",
    "                            target_record=target_record,\n",
    "                            target_name=target_name,\n",
    "                            target_col=target_col,\n",
    "                            train_months_used=int(k),\n",
    "                            x_cols=X_COLS_MULT,\n",
    "                            train_windows_full=train_windows_full,\n",
    "                            train_windows_used=train_windows_used,\n",
    "                            window_pack=pack,\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "    valid_months = [int(k) for k in TRAIN_MONTHS_SWEEP if int(k) not in set(skipped_months)]\n",
    "    for k in valid_months:\n",
    "        method_set = set(r[\"Method\"] for r in rows if int(r[\"TrainMonthsUsed\"]) == int(k))\n",
    "        if method_set != set(METHODS):\n",
    "            raise AssertionError(f\"Incomplete methods for target={tgt}, k={k}: got={sorted(method_set)} expected={METHODS}\")\n",
    "\n",
    "    return rows\n",
    "\n",
    "# ============================================================\n",
    "# Run protocol (MLflow nesting + tqdm)\n",
    "# ============================================================\n",
    "all_rows = []\n",
    "\n",
    "with mlflow.start_run(run_name=\"cross_station_train_months_sweep_torch_leakage_fixed\") as top:\n",
    "    mlflow.log_params({\n",
    "        \"data_dir\": DATA_DIR,\n",
    "        \"geo_json\": GEO_JSON_PATH,\n",
    "        \"train_days\": TRAIN_DAYS,\n",
    "        \"val_days\": VAL_DAYS,\n",
    "        \"train_months_sweep\": \",\".join(str(k) for k in TRAIN_MONTHS_SWEEP),\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"horizon\": HORIZON,\n",
    "        \"lstm_units\": LSTM_UNITS,\n",
    "        \"embed_dim\": EMBED_DIM,\n",
    "        \"lr\": LR,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"device\": str(device),\n",
    "        \"n_stations\": len(stations),\n",
    "        \"leakage_fix\": \"split_then_impute_per_split\",\n",
    "    })\n",
    "    config_snapshot = {\n",
    "        \"seed\": SEED,\n",
    "        \"train_days\": TRAIN_DAYS,\n",
    "        \"val_days\": VAL_DAYS,\n",
    "        \"train_months_sweep\": TRAIN_MONTHS_SWEEP,\n",
    "        \"seq_len\": SEQ_LEN,\n",
    "        \"horizon\": HORIZON,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"max_epochs\": MAX_EPOCHS,\n",
    "        \"lr\": LR,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "        \"reduce_lr_patience\": REDUCE_LR_PATIENCE,\n",
    "        \"lstm_units\": LSTM_UNITS,\n",
    "        \"embed_dim\": EMBED_DIM,\n",
    "        \"geo_dim\": GEO_DIM,\n",
    "        \"x_cols_mult\": X_COLS_MULT,\n",
    "        \"methods\": METHODS,\n",
    "        \"device\": str(device),\n",
    "    }\n",
    "    snap_path = os.path.join(RESULTS_DIR, \"config_snapshot.json\")\n",
    "    with open(snap_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(config_snapshot, f, indent=2)\n",
    "    mlflow.log_artifact(snap_path, artifact_path=\"config\")\n",
    "\n",
    "    for source_record in tqdm(station_data, desc=\"Source station folds\"):\n",
    "        src_station = source_record[\"station\"]\n",
    "\n",
    "        with mlflow.start_run(run_name=f\"source={src_station}\", nested=True):\n",
    "            src_pack = train_source_models(source_record)\n",
    "            mlflow.log_metrics({\n",
    "                \"src_backbone_train_time_sec\": float(src_pack[\"backbone_train_time\"]),\n",
    "                \"src_hybrid_train_time_sec\": float(src_pack[\"hybrid_train_time\"]),\n",
    "            })\n",
    "\n",
    "            # log source model state dicts\n",
    "            tmp_dir = os.path.join(RESULTS_DIR, \"tmp_models\")\n",
    "            os.makedirs(tmp_dir, exist_ok=True)\n",
    "\n",
    "            paths = {\n",
    "                f\"{src_station}_source_backbone.pt\": src_pack[\"backbone_model\"],\n",
    "                f\"{src_station}_source_hybrid.pt\": src_pack[\"hybrid_model\"],\n",
    "            }\n",
    "            for fname, model in paths.items():\n",
    "                p = os.path.join(tmp_dir, fname)\n",
    "                torch.save(model.state_dict(), p)\n",
    "                mlflow.log_artifact(p, artifact_path=\"source_models\")\n",
    "\n",
    "            targets = [d for d in station_data if d[\"station\"] != src_station]\n",
    "            for target_record in tqdm(targets, desc=f\"Targets for {src_station}\", leave=False):\n",
    "                all_rows.extend(eval_target_sweep(src_station, src_pack, target_record))\n",
    "\n",
    "# Save results\n",
    "results_df = pd.DataFrame(all_rows)\n",
    "required_cols = [\n",
    "    \"TargetName\", \"TargetCol\", \"SourceStation\", \"TargetStation\", \"Method\", \"TrainMonthsUsed\",\n",
    "    \"MAE\", \"RMSE\", \"MSE\", \"MAPE\", \"sMAPE\", \"R2\", \"EVS\", \"TrainTimeSec\"\n",
    "]\n",
    "for c in required_cols:\n",
    "    if c not in results_df.columns:\n",
    "        results_df[c] = np.nan\n",
    "results_df = results_df[required_cols]\n",
    "metrics_path = os.path.join(RESULTS_DIR, \"metrics_long.csv\")\n",
    "results_df.to_csv(metrics_path, index=False)\n",
    "print(\"Saved:\", metrics_path)\n",
    "\n",
    "# Aggregate artifacts\n",
    "summary = (\n",
    "    results_df\n",
    "    .groupby([\"Method\", \"TrainMonthsUsed\"])[[\"MAE\", \"RMSE\", \"MSE\", \"MAPE\", \"sMAPE\", \"R2\", \"EVS\", \"TrainTimeSec\"]]\n",
    "    .agg([\"mean\", \"std\"])\n",
    "    .reset_index()\n",
    ")\n",
    "summary_path = os.path.join(RESULTS_DIR, \"summary_by_method.csv\")\n",
    "summary.to_csv(summary_path, index=False)\n",
    "\n",
    "pivot = (\n",
    "    results_df\n",
    "    .pivot_table(index=[\"SourceStation\", \"TargetStation\", \"TrainMonthsUsed\"], columns=\"Method\", values=\"MAE\")\n",
    "    .reset_index()\n",
    ")\n",
    "if \"GeoAware_TL\" in pivot.columns and \"TL\" in pivot.columns:\n",
    "    pivot[\"GeoAware_minus_TL_MAE\"] = pivot[\"GeoAware_TL\"] - pivot[\"TL\"]\n",
    "if \"GeoAware_TL\" in pivot.columns and \"Scratch\" in pivot.columns:\n",
    "    pivot[\"GeoAware_minus_Scratch_MAE\"] = pivot[\"GeoAware_TL\"] - pivot[\"Scratch\"]\n",
    "delta_path = os.path.join(RESULTS_DIR, \"deltas.csv\")\n",
    "pivot.to_csv(delta_path, index=False)\n",
    "\n",
    "with mlflow.start_run(run_name=\"analysis_summary_torch\"):\n",
    "    mlflow.log_artifact(metrics_path, artifact_path=\"results\")\n",
    "    mlflow.log_artifact(summary_path, artifact_path=\"results\")\n",
    "    mlflow.log_artifact(delta_path, artifact_path=\"results\")\n",
    "    mean_mae = results_df.groupby(\"Method\")[\"MAE\"].mean().to_dict()\n",
    "    mlflow.log_metrics({f\"mean_mae_{k}\": float(v) for k, v in mean_mae.items()})\n",
    "\n",
    "results_df.head()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}